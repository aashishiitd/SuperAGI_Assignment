{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "E3d_yeZhrYiC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Q/A Assignement**"
      ],
      "metadata": {
        "id": "E3d_yeZhrYiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) You train Logistic Regression with a certain set of features and learn weights $w_0$, $w_1$ till $w_n$.\n",
        "Feature $n$ gets weight $w_n$ at the end of training. Say you now create a new dataset where you duplicate feature $n$ into feature $(n+1)$ and retrain a new model. Suppose this new model weights are $w_{new_0}$, $w_{new_1}$ till $w_{new_n}$, $w_{new_{n+1}}$. What is the likely relationship between $w_{new_0}$, $w_{new_1}$ , $w_{new_n}$,  and $w_{new_{n+1}}$?\n"
      ],
      "metadata": {
        "id": "I48dnPUKrNkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - In Logistic Regression, if we duplicate a feature, the weights of the original and duplicated features will likely be distributed between the two. This is because the two features are perfectly correlated, and the model can distribute the weight between them while still achieving the same result.\n",
        "\n",
        "So, if wn was the weight of the original feature n, after duplication, the weights $w_{new_n}$ and $w_{new_n+1}$ of the original and duplicated features will likely sum up to $w_{new_n}$.\n",
        "\n",
        "This is not a strict rule, as the exact distribution can depend on the specifics of the training process, but it's a common outcome.\n",
        "\n",
        "Duplicating features doesn't add any new information to the model, and can lead to issues like multicollinearity, which can make the model's estimates less stable."
      ],
      "metadata": {
        "id": "PbnX_PteZxq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. You currently have an email marketing template A and you want to replace it with a better template. A is the control_template. You also test email templates B, C, D, E. You send exactly 1000 emails of each template to different random users. You wish to figure out what email gets the highest click through rate. Template A gets 10% click through rate (CTR), B gets 7% CTR, C gets 8.5% CTR, D gets 12% CTR and E gets 14% CTR. You want to run your multivariate test till you get 95% confidence in a conclusion. Which of the following is true?\n",
        "    1. We have too little data to conclude that A is better or worse than any other template with 95% confidence.\n",
        "    2. E is better than A with over 95% confidence, B is worse than A with over 95% confidence. You need to run the test for longer to tell where C and D compare to A with 95% confidence.\n",
        "    3. Both D and E are better than A with 95% confidence. Both B and C are worse than A with over 95% confidence"
      ],
      "metadata": {
        "id": "lfnb6VxgcBL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans - Option 2. E is better than A with over 95% confidence. B is worse than A with over 95% confidence. You need to run the test for longer to tell where C and D compare to A with 95% confidence."
      ],
      "metadata": {
        "id": "9DSej7rfcHAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) You have $m$ training examples and $n$ features. Your feature vectors are however sparse and average number of non-zero entries in each train example is $k$ and $k << n$. What is the approximate computational cost of each gradient descent iteration of logistic regression in modern well written packages?"
      ],
      "metadata": {
        "id": "eioa0Sh3enrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans- The computational cost of each gradient descent iteration of logistic regression is typically proportional to the number of non-zero entries in the feature vectors. This is because modern packages like Scikit-learn etc, take advantage of sparse data structures and operations.\n",
        "\n",
        "In this case, if we have $m$ training examples and each example has on average $k$ non-zero entries, then the computational cost of each gradient descent iteration is approximately $O(m*k)$.\n",
        "\n",
        "This is significantly less than the $O(m*n)$ cost that would be incurred if all entries in the feature vectors were non-zero and the sparsity of the data was not exploited.\n",
        "\n"
      ],
      "metadata": {
        "id": "5YK42Nr8e85r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) We are interested in building a high quality text classifier that categorizes news stories into 2 categories - information and entertainment. We want the classifier to stick with predicting the better among these two categories (this classifier won't try to predict a percent score for these two categories). You have already trained V1 of a classifier with 10,000 news stories from the New York Times, which is one of 1000 new sources we would like the next version of our classifier (let's call it V2) to correctly categorize stories for. You would like to train a new classifier with the original 10,000 New York Times news stories and an additional 10,000 different news stories and no more. Below are approaches to generating the additional 10,000 pieces of train data for training V2.\n",
        "\n",
        "    1. Run our V1 classifier on 1 Million random stories from the 1000 news sources. Get the 10k stories where the V1 classifier’s output is closest to the decision boundary and get these examples labeled.\n",
        "    2. Get 10k random labeled stories from the 1000 news sources we care about.\n",
        "    3. Pick a random sample of 1 million stories from 1000 news sources and have them labeled. Pick the subset of 10k stories where the V1 classifier’s output is both wrong and farthest away from the decision boundary.\n",
        "    \n",
        "  Ignore the difference in costs and effort in obtaining train data using the different methods described above. In terms of pure accuracy of classifier V2 when classifying a bag of new articles from 1000 news sources, what is likely to be the value of these different methods?How do you think the models will rank based on their accuracy?"
      ],
      "metadata": {
        "id": "d56uBXTCfnM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans-\n",
        "\n",
        "In terms of pure accuracy, the ranking will be: 2 > 3 > 1.\n",
        "\n",
        "**Running V1 classifier on 1 Million random stories and picking 10k stories closest to the decision boundary**: This approach is likely to improve the classifier's performance on ambiguous cases that lie near the decision boundary. However, it might not significantly improve the overall accuracy as it doesn't necessarily cover a wide range of examples.\n",
        "\n",
        "**Getting 10k random labeled stories from the 1000 news sources**: This approach provides a more diverse set of training examples, which can help the classifier generalize better to unseen data. It's likely to improve the overall accuracy more than the first approach.\n",
        "\n",
        "**Picking a random sample of 1 million stories and selecting 10k stories where V1 classifier’s output is wrong and farthest from the decision boundary**: This approach focuses on the most challenging examples for the current classifier. It's likely to improve the classifier's performance on these difficult cases, but it might not generalize well to typical cases."
      ],
      "metadata": {
        "id": "fNmsE_GbgfMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) You wish to estimate the probability, $p$ that a coin will come up heads, since it may not be a fair coin. You toss the coin $n$ times and it comes up heads $k$ times. You use the following three methods to estimate $p$\n",
        "      \n",
        "      1. Maximum Likelihood estimate (MLE)\n",
        "      2. Bayesian Estimate: Here you assume a continuous distribution uniform prior to $p$ from $[0,1]$ (i.e. the probability density function for the value of $p$ is uniformly $1$ inside this range and $0$ outside. Our estimate for $p$ will be the expected value of the posterior distribution of $p$. The posterior distribution is conditioned on these observations.\n",
        "      3. Maximum a posteriori (MAP) estimate: Here you assume that the prior is the same as (b). But we are interested in the value of $p$ that corresponds to the mode of the posterior distribution.\n",
        "    \n",
        "  What are the estimates?"
      ],
      "metadata": {
        "id": "fi1iwMdxojQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans -\n",
        "(a) As the probability $p$ will follow Bernoulli's distribution. Now for its pmf we can write the likelihood function of it and take the log of it and finally if we take the derivative of the whole we will get $p=K/N$. Now, we take the double derivative and verify it is a maxima. Hence, $p=K/N$.\n",
        "\n",
        "(b)"
      ],
      "metadata": {
        "id": "f_NjUqeygatF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding Assignment: Implementation and Optimization of GPT-2 Model"
      ],
      "metadata": {
        "id": "QhHhN0DTpC6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK1**"
      ],
      "metadata": {
        "id": "2-xGRUjDpOhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "class GPT2Config:\n",
        "    def __init__(self, vocab_size=50257, n_positions=1024, n_ctx=1024, n_embd=768,\n",
        "                 n_layer=12, n_head=12, layer_norm_epsilon=1e-5, initializer_range=0.02,\n",
        "                 embd_pdrop=0.1, resid_pdrop=0.1, attn_pdrop=0.1, bos_token_id=50256,\n",
        "                 eos_token_id=50256, model_type=\"gpt2\", activation_function=\"gelu_new\",\n",
        "                 architectures=[\"GPT2LMHeadModel\"], n_inner=None,\n",
        "                 reorder_and_upcast_attn=False, scale_attn_by_inverse_layer_idx=False,\n",
        "                 scale_attn_weights=True, summary_activation=None,\n",
        "                 summary_first_dropout=0.1, summary_proj_to_labels=True,\n",
        "                 summary_type=\"cls_index\", summary_use_proj=True,\n",
        "                 task_specific_params={\"text-generation\": {\"do_sample\": True, \"max_length\": 50}},\n",
        "                 transformers_version=\"4.35.2\", use_cache=True):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_positions = n_positions\n",
        "        self.n_ctx = n_ctx\n",
        "        self.n_embd = n_embd\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.layer_norm_epsilon = layer_norm_epsilon\n",
        "        self.initializer_range = initializer_range\n",
        "        self.embd_pdrop = embd_pdrop\n",
        "        self.resid_pdrop = resid_pdrop\n",
        "        self.attn_pdrop = attn_pdrop\n",
        "        self.bos_token_id = bos_token_id\n",
        "        self.eos_token_id = eos_token_id\n",
        "        self.model_type = model_type\n",
        "        self.activation_function = activation_function\n",
        "        self.architectures = architectures\n",
        "        self.n_inner = n_inner\n",
        "        self.reorder_and_upcast_attn = reorder_and_upcast_attn\n",
        "        self.scale_attn_by_inverse_layer_idx = scale_attn_by_inverse_layer_idx\n",
        "        self.scale_attn_weights = scale_attn_weights\n",
        "        self.summary_activation = summary_activation\n",
        "        self.summary_first_dropout = summary_first_dropout\n",
        "        self.summary_proj_to_labels = summary_proj_to_labels\n",
        "        self.summary_type = summary_type\n",
        "        self.summary_use_proj = summary_use_proj\n",
        "        self.task_specific_params = task_specific_params\n",
        "        self.transformers_version = transformers_version\n",
        "        self.use_cache = use_cache\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, attention_mask=None, dropout=None):\n",
        "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))\n",
        "    dk = k.size(-1)\n",
        "    scaled_attention_logits = matmul_qk / math.sqrt(dk)\n",
        "\n",
        "    if attention_mask is not None:\n",
        "        scaled_attention_logits += (attention_mask * -1e9)\n",
        "\n",
        "    attention_weights = nn.Softmax(dim=-1)(scaled_attention_logits)\n",
        "    if dropout is not None:\n",
        "        attention_weights = dropout(attention_weights)\n",
        "\n",
        "    output = torch.matmul(attention_weights, v)\n",
        "    return output, attention_weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = nn.Dropout(config.attn_pdrop)\n",
        "        assert self.n_embd % self.n_head == 0\n",
        "\n",
        "        self.wq = nn.Linear(self.n_embd, self.n_embd)\n",
        "        self.wk = nn.Linear(self.n_embd, self.n_embd)\n",
        "        self.wv = nn.Linear(self.n_embd, self.n_embd)\n",
        "        self.dense = nn.Linear(self.n_embd, self.n_embd)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = x.view(batch_size, -1, self.n_head, self.n_embd // self.n_head)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, v, k, q, mask):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, _ = scaled_dot_product_attention(q, k, v, mask, self.dropout)\n",
        "        scaled_attention = scaled_attention.permute(0, 2, 1, 3)\n",
        "\n",
        "        concat_attention = scaled_attention.contiguous().view(batch_size, -1, self.n_embd)\n",
        "        output = self.dense(concat_attention)\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(config)\n",
        "        self.norm1 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "        self.norm2 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, config.n_embd * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(config.n_embd * 4, config.n_embd),\n",
        "            nn.Dropout(config.resid_pdrop),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.attention(x, x, x, mask)\n",
        "        x = x + self.dropout(attn_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = x + self.dropout(ffn_output)\n",
        "        x = self.norm2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GPT2, self).__init__()\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        self.h = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "        self.fc = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        position_ids = torch.arange(0, input_ids.size(-1), dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "\n",
        "        input_embeds = self.wte(input_ids)\n",
        "        position_embeds = self.wpe(position_ids)\n",
        "\n",
        "        hidden_states = input_embeds + position_embeds\n",
        "        hidden_states = self.drop(hidden_states)\n",
        "\n",
        "        for block in self.h:\n",
        "            hidden_states = block(hidden_states, attention_mask)\n",
        "\n",
        "        hidden_states = self.ln_f(hidden_states)\n",
        "        logits = self.fc(hidden_states)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "mygpQwVcvcpl"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = GPT2(GPT2Config())\n",
        "# model.load_state_dict(torch.load(\"gpt2_weights.pth\"))\n",
        "\n",
        "# input_ids = torch.tensor([[50256, 345, 262, 263, 1818, 287, 262, 2635, 11, 290, 262, 1898, 287, 7526, 11, 423, 262, 2635, 13, 198, 198, 198, 40, 858, 262, 1578, 764, 11, 290, 262, 1898, 287, 7526, 764]])\n",
        "# attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     outputs = model(input_ids, attention_mask=attention_mask)"
      ],
      "metadata": {
        "id": "sObubVtQ1aKg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import GPT2Config\n",
        "\n",
        "# # Get the configuration of the pre-trained model\n",
        "# config = GPT2Config.from_pretrained('gpt2')\n",
        "# print(config)\n",
        "\n",
        "# #(self, vocab_size=50257, n_positions=1024, n_ctx=1024, n_embd=768,\n",
        "#                 #  n_layer=12, n_head=12, layer_norm_epsilon=1e-05, initializer_range=0.02,\n",
        "#                 #  embd_pdrop=0.1, resid_pdrop=0.1):"
      ],
      "metadata": {
        "id": "tvQg0lL61PjJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 2**\n"
      ],
      "metadata": {
        "id": "TIT8zanKpFsq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qpVwJZuYYlxN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pC_aKcZear4U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
